{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d84c6e2",
   "metadata": {},
   "source": [
    "## Importing libraries:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "\n",
    "DATA_DIR = \"../raw_data/\"\n",
    "ZIP_PATH = os.path.join(DATA_DIR, \"creditcardfraud.zip\")\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f436e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(ZIP_PATH):\n",
    "    url = \"https://www.kaggle.com/api/v1/datasets/download/mlg-ulb/creditcardfraud\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(ZIP_PATH, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(\"Succesfully downloaded.\")\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "if not os.path.exists(CSV_PATH) and os.path.exists(ZIP_PATH):\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "    print(\"Data extracted succesfully.\")\n",
    "\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(\"DataFrame loaded succesfully.\")\n",
    "else:\n",
    "    print(\"CSV file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d22f1b",
   "metadata": {},
   "source": [
    "To help understand the problems we are facing, the following image helps to determine the best metric to be used based on the problem we are solving:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dde6b5",
   "metadata": {},
   "source": [
    "![metrics.png](https://machinelearningmastery.com/wp-content/uploads/2019/12/How-to-Choose-a-Metric-for-Imbalanced-Classification-latest.png)\n",
    "\n",
    "source: [MachineLearningMastery](https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # Display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3ecb08",
   "metadata": {},
   "source": [
    "### Stratified splitting:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into features and target variable\n",
    "# 'Class' is the target variable indicating fraud (1) or not fraud (0)\n",
    "# The rest of the columns are features used for prediction\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17282e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca42818",
   "metadata": {},
   "source": [
    "### Oversample/undersample before or after splitting data?\n",
    "---\n",
    "Main rule: **Always** after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4cb5c0",
   "metadata": {},
   "source": [
    "### Feature Scaling:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30aecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Creating function for scaling\n",
    "def Standard_Scaler(df: pd.DataFrame, col_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes the features in the DataFrame using StandardScaler.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the features to be scaled.\n",
    "        col_names (list): List of column names to be scaled.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the specified columns scaled.\n",
    "    \"\"\"\n",
    "    features = df[col_names]\n",
    "    scaler = StandardScaler().fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "    df[col_names] = features\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fd814",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Standard_Scaler (X_train, ['Amount'])\n",
    "X_test = Standard_Scaler (X_test, ['Amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef5a61",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[\"Genuine\",\"Fraud\"]\n",
    "\n",
    "fraud_or_not = df[\"Class\"].value_counts().tolist()\n",
    "values = [fraud_or_not[0], fraud_or_not[1]]\n",
    "\n",
    "fig = px.pie(values=df['Class'].value_counts(), names=labels , width=700, height=400, color_discrete_sequence=[\"skyblue\",\"black\"]\n",
    "             ,title=\"Fraud vs Genuine transactions\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#We are going to ensure that we have the same splits of the data every time. \n",
    "#We can ensure this by creating a KFold object, kf, and passing cv=kf instead of the more common cv=5.\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=13)\n",
    "#cross_val_score(rf, X_train, y_train, cv=kf, scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "score = cross_val_score(rf, X_train, y_train, cv=kf, scoring='recall')\n",
    "print(\"Cross Validation Recall scores are: {}\".format(score))\n",
    "print(\"Average Cross Validation Recall score: {}\".format(score.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
